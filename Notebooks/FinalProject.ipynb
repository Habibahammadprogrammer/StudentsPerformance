{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3216f8b8",
   "metadata": {},
   "source": [
    "# Student Performance Insights : From Cleaning to Clustering to Classification\n",
    "## Name:Habiba abdullah said hammad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cddedabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036eac4",
   "metadata": {},
   "source": [
    "### Libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfa26560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472deaeb",
   "metadata": {},
   "source": [
    "### Data Prepration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c414cb74",
   "metadata": {},
   "source": [
    "Ok let's first start with downloading the dataset from UCI ML Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f76ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ucimlrepo -q\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "\n",
    "ds = fetch_ucirepo(id=320)\n",
    "X = ds.data.features\n",
    "y = ds.data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3ff43",
   "metadata": {},
   "source": [
    "Note that the server may be down which can cause problems of showing the dataset, however the dataset is already saved as a csv so if the server is down use that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20294f7c",
   "metadata": {},
   "source": [
    "Let's look at the meta data of the dataset \n",
    "meta data is data about data so we will have a deeper understanding of the dataset itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2494a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a9fbf",
   "metadata": {},
   "source": [
    "This dataset is a *merged* view of the students math scores and porteguse scores\n",
    "Ok looks like there are 649 students and 30 features. \n",
    "it also claims that there is no missing values or even symbols for missing values.\n",
    "Let's take a look at the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\",None)\n",
    "ds.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef751775",
   "metadata": {},
   "source": [
    "Ok so most features here are integers except for \n",
    "- school \n",
    "- sex \n",
    "- famsize\n",
    "- Mjob (Mother's Job)\n",
    "- Fjob (Father's Job)\n",
    "- Guardian \n",
    "\n",
    "\n",
    "So these would need encoding and given that there are binary features we can one-hot encode them for consistency.\n",
    "Now let's look at the dataset itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1bf9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb8a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#will need this is in visulaization and EDA\n",
    "X_original=X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b220243",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3e48d3",
   "metadata": {},
   "source": [
    "Ok for reproduciatlity , I will save the dataset as csv files in the Data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81641981",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv(\"E:\\Summer Semester 2\\Training\\Final Project\\Data\\student_features.csv\",index=False)\n",
    "y.to_csv(\"E:\\Summer Semester 2\\Training\\Final Project\\Data\\student_targets.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ebcac2",
   "metadata": {},
   "source": [
    "If for any reason this doens't work , the files are avalibale as csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08792273",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.read_csv(\"E:\\Summer Semester 2\\Training\\Final Project\\Data\\student_features.csv\")\n",
    "y=pd.read_csv(\"E:\\Summer Semester 2\\Training\\Final Project\\Data\\student_targets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea049216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  ...  \\\n",
       "0     GP   F   18       U     GT3       A     4     4  at_home   teacher  ...   \n",
       "1     GP   F   17       U     GT3       T     1     1  at_home     other  ...   \n",
       "2     GP   F   15       U     LE3       T     1     1  at_home     other  ...   \n",
       "3     GP   F   15       U     GT3       T     4     2   health  services  ...   \n",
       "4     GP   F   16       U     GT3       T     3     3    other     other  ...   \n",
       "\n",
       "  higher internet  romantic  famrel  freetime goout Dalc Walc health absences  \n",
       "0    yes       no        no       4         3     4    1    1      3        4  \n",
       "1    yes      yes        no       5         3     3    1    1      3        2  \n",
       "2    yes      yes        no       4         3     2    2    3      3        6  \n",
       "3    yes      yes       yes       3         2     2    1    1      5        0  \n",
       "4    yes       no        no       4         3     2    1    2      5        0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a899fe0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   G1  G2  G3\n",
       "0   0  11  11\n",
       "1   9  11  11\n",
       "2  12  13  12\n",
       "3  14  14  14\n",
       "4  11  13  13"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98539e6",
   "metadata": {},
   "source": [
    "Now that that's setteled, Let's check for duplicates and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55237245",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_count=X.duplicated().sum()\n",
    "dup_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b857a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_count=X.isnull().sum()\n",
    "missing_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cab7e10",
   "metadata": {},
   "source": [
    "Ok so no duplicates or null values , let's now check for outliers , I will use the IQR method since it is the most simple and when plotted in a box plot it makes it easier to detect outliers while still vieweing the distribution of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560c059c",
   "metadata": {},
   "source": [
    "This function will check for outliers for all the numeric values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd9837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df, threshold=1.5):\n",
    "    outlier_summary = {}\n",
    "\n",
    "    for col in df.select_dtypes(include=[\"int64\"]).columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower = Q1 - threshold * IQR\n",
    "        upper = Q3 + threshold * IQR\n",
    "\n",
    "        outliers = df[(df[col] < lower) | (df[col] > upper)]\n",
    "        outlier_summary[col] = {\n",
    "            \"outlier_count\": outliers.shape[0],\n",
    "            \"lower_bound\": lower,\n",
    "            \"upper_bound\": upper,\n",
    "            \"min_value\": df[col].min(),\n",
    "            \"max_value\": df[col].max()\n",
    "        }\n",
    "    return pd.DataFrame(outlier_summary).T.sort_values(\"outlier_count\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abae3a5c",
   "metadata": {},
   "source": [
    "And this function is to plot boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outliers_boxplots(df):\n",
    "    numeric_cols = df.select_dtypes(include=[\"int64\"]).columns\n",
    "    n = len(numeric_cols)\n",
    "\n",
    "    rows = (n // 2) + (n % 2)\n",
    "    plt.figure(figsize=(12, 4*rows))\n",
    "\n",
    "    for i, col in enumerate(numeric_cols, 1):\n",
    "        plt.subplot(rows, 2, i)\n",
    "        sns.boxplot(x=df[col], color=\"skyblue\")\n",
    "        plt.title(f\"Boxplot of {col}\")\n",
    "        plt.xlabel(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1113ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_report=detect_outliers_iqr(X)\n",
    "outlier_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e34bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers_boxplots(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3e5714",
   "metadata": {},
   "source": [
    "Looking at the features , absences at first glance look a little off but after some thinking , yes some students can have be absent for most of the year (Reasons could be Travel) but to not let it affect the model , we can replace the values beyond a threshold with the nearest non-outlier value.\n",
    "\n",
    "Now for featues like Famrel , FreeTime and Dalc (Daily alcoholic consumption) have some outliers but theya re not significant and are also logical. \n",
    "\n",
    "Finally , the failures feature , at first glance , looks like it needs to be dropped right away but actually it is an ordinal Categorical value with values ranging from 0 (no past failures) to 3 (3 or more past failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef6e694",
   "metadata": {},
   "source": [
    "Let's cap absences at 95th percentile for top 5% of values above 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab4647",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper=X['absences'].quantile(0.95)\n",
    "X['absences']=X['absences'].clip(upper=upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a7bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = X['absences'].quantile(0.25)\n",
    "Q3 = X['absences'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Lower Bound for absences: {lower_bound}\")\n",
    "print(f\"Upper Bound for absences: {upper_bound}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32567dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_lower_after = (X['absences'] < lower_bound).sum()\n",
    "outliers_upper_after = (X['absences'] > upper_bound).sum()\n",
    "total_outliers_after = outliers_lower_after + outliers_upper_after\n",
    "\n",
    "print(f\"Lower Outliers after capping: {outliers_lower_after}\")\n",
    "print(f\"Upper Outliers after capping: {outliers_upper_after}\")\n",
    "print(f\"Total Outliers after capping: {total_outliers_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e9a114",
   "metadata": {},
   "source": [
    "Now the maximum value is 12 absences. Let's go on to the target features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_target_report=detect_outliers_iqr(y)\n",
    "outlier_target_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd7bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_outliers_boxplots(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6394d5d9",
   "metadata": {},
   "source": [
    "So looking at the target features , there seemes to be no issue since they are withtin the range of 0-20 which is stated in the 'variable_info' in the metadata. Yes there is some outliers but they are logical since students can get a zero or get a below average score therfore the target dataframe stays as is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c9cd0",
   "metadata": {},
   "source": [
    "Data Quality Report Can be found in the Reports Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465d574c",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30715007",
   "metadata": {},
   "source": [
    "First step in Data Transformation is Encoding \n",
    "I wil use one-hot encoding for categorical and binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe1dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Categorical_columns=['school','address','famsize','Pstatus','Mjob','Fjob','reason','guardian']\n",
    "binary_columns=['sex','schoolsup','famsup','paid','activities','nursery','higher','internet','romantic']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057f0e4b",
   "metadata": {},
   "source": [
    "First let's handle the binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b920da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in binary_columns:\n",
    "    X[col]=X[col].map({\"yes\":1,\"no\":0}).fillna(X[col])\n",
    "    if col==\"sex\":\n",
    "        X[col]=X[col].map({\"F\":0,\"M\":1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de49976",
   "metadata": {},
   "source": [
    "Now onto the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1cbb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.get_dummies(X,columns=Categorical_columns,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b07276",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb1759",
   "metadata": {},
   "source": [
    "Now that the encoding is settled , let's go on to scaling for numeric features. Since I will be using KNN it is important for all the numeric features to be scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ee85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "numeric_features=['age','Medu','Fedu','traveltime','studytime','failures','famrel','freetime','goout','Dalc','Walc','health','absences']\n",
    "scaler=StandardScaler()\n",
    "X[numeric_features]=scaler.fit_transform(X[numeric_features])\n",
    "X[numeric_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd1560",
   "metadata": {},
   "source": [
    "Now all the values range from -3 to +3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693f349",
   "metadata": {},
   "source": [
    "Now We can go on to feature engineering.\n",
    "We will do two things \n",
    "1. Average of G1-G3 \n",
    "2. Attendance Proxy\n",
    "3. Family academic Enviroment\n",
    "4. Support Index\n",
    "5. Lifestyle Score\n",
    "6. Risk Group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ebe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"G1\"]=y['G1']\n",
    "X['G2']=y['G2']\n",
    "X['G3']=y['G3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21533542",
   "metadata": {},
   "source": [
    "Attendence Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a34407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"attendance_rate\"] = 1 - (X_original['absences'] / X_original['absences'].max())\n",
    "X['attendance_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3965708",
   "metadata": {},
   "source": [
    "Risk Group (for classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e4825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_group(g3):\n",
    "    if g3<=9: return \"High\"\n",
    "    if g3<=14: return \"Medium\"\n",
    "    return \"Low\"\n",
    "X['risk_group']=y['G3'].apply(risk_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d774e14",
   "metadata": {},
   "source": [
    "Average of G1-G3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa0eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['avg_grade']=(X['G1']+X['G2']+X['G3'])/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bf0ead",
   "metadata": {},
   "source": [
    "Now we need to consider leakage \n",
    "having G1 and G2 would make it easy for the model to predict G3 and can cause leakage \n",
    "so to surpass this we will have 2 dataframes \n",
    "one with G1 and G2 \n",
    "nd one without G1 and G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60922a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with=X.drop(columns=['G3','risk_group'])\n",
    "df_with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86e5c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols=[\"G1\",'G2','G3','avg_grade','risk_group']\n",
    "df_without=X.drop(columns=[c for c in drop_cols if c in X.columns])\n",
    "df_without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a97d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification\n",
    "y_with_cls=X['risk_group']\n",
    "y_without_cls=X['risk_group']\n",
    "#Regression \n",
    "y_with_rg=X['G3']\n",
    "y_without_rg=X['G3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f678a5d",
   "metadata": {},
   "source": [
    "Ok let's confirm we actually did this step correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d27c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98708bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268dd83a",
   "metadata": {},
   "source": [
    "Here are the G3 grades but as categories with the previous function risk_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12545f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_with_cls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_without_cls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac91cad",
   "metadata": {},
   "source": [
    "Here are the G3 grades (actual values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d5f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_without_rg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3856da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_with_rg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8e4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"avg_grade\" in df_with.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38422bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"avg_grade\" in df_without.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef290da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ad75e5",
   "metadata": {},
   "source": [
    "All of the sanity checks are correct. Now the dataset is ready for EDA and visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a8ab3",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ca0a0",
   "metadata": {},
   "source": [
    "Now We will try to explore the data even more. For EDA I will use a merged df with the original features and the target variables called df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd6b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr=X_original.copy()\n",
    "df_corr['G1']=y['G1']\n",
    "df_corr['G2']=y['G2']\n",
    "df_corr['G3']=y['G3']\n",
    "num_features=numeric_features+['G1','G2','G3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d95d9c",
   "metadata": {},
   "source": [
    "Let's start with descriptive stats table for the  numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59d3373",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_summary=(\n",
    "    df_corr[num_features].describe(percentiles=[0.25,0.5,0.75])\n",
    "    .T\n",
    "    .rename(columns={'25%':'q1','50%':'q2','75%':'q3'})\n",
    "    .round(2)\n",
    ")\n",
    "num_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd807a31",
   "metadata": {},
   "source": [
    "And let's not forget the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_summary=pd.DataFrame({\n",
    "    'unique_values':df_corr[Categorical_columns].nunique(),\n",
    "    'most_frequent':df_corr[Categorical_columns].mode().iloc[0],\n",
    "    'freq_count':df_corr[Categorical_columns].apply(lambda x:x.value_counts().iloc[0]),\n",
    "                        })\n",
    "cat_summary['freq_percent']=(cat_summary['freq_count']/len(df_corr)*100).round(2)\n",
    "cat_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf911a36",
   "metadata": {},
   "source": [
    "Now let's do Correlation analysis\n",
    "notice that this will be done for the numeric features only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa3abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=df_corr[num_features].corr()\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr,annot=True,fmt=\".2f\",cmap=\"coolwarm\",cbar=True,square=True)\n",
    "plt.title(\"Correlation Heatmap of Numeric Features\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf67c67",
   "metadata": {},
   "source": [
    "Ok so looks like G3 has a  strong positivie correlation with G1(0.83) and G2(0.92)\n",
    "it looks like it has a strong negative correlation failures (-0.39) which means the more failures the lower the value of G3/ \n",
    "Suprisingly, Study time has a weak positive correlation and absences has a very weak or no correlation.\n",
    "So the important predictors for G3 is G1 , G2 and failures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163ea4f2",
   "metadata": {},
   "source": [
    "Now let's do Group Comparisons \n",
    "- Study time vs outcomes \n",
    "- absences vs outcomes \n",
    "- School support vs outcomes \n",
    "- failures vs outcomes\n",
    "- sex vs outcomes \n",
    "- lifestyle score vs outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776b0d8c",
   "metadata": {},
   "source": [
    "Study time vs outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d070568",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,8))\n",
    "sns.boxplot(x='studytime',y='G3',data=df_corr)\n",
    "plt.title(\"Final Grade vs Study Time\",fontsize=14)\n",
    "plt.xlabel(\"Study time (1=Low , 4=High)\")\n",
    "plt.ylabel(\"Final Grade\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d972df8",
   "metadata": {},
   "source": [
    "So there is a slight difference between different study times and final grade  but it isn't that strong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142575ca",
   "metadata": {},
   "source": [
    "Absences vs outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b75fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x='absences',y='G3',data=df_corr,alpha=0.6)\n",
    "plt.title(\"Final Grade vs Absences\",fontsize=14)\n",
    "plt.ylabel(\"Absences\")\n",
    "plt.xlabel(\"Final Grade\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06852d7c",
   "metadata": {},
   "source": [
    "Looks like most points are clusterd around low absences (0-15) and final grades (5-15), however there is a weak negative correlation and the relationship is not that strong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6937a",
   "metadata": {},
   "source": [
    "School Support vs outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be01db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x='schoolsup',y='G3',data=df_corr)\n",
    "plt.title(\"Final Grade vs School Support\",fontsize=14)\n",
    "plt.xlabel(\"School Support (Yes/No)\")\n",
    "plt.ylabel(\"Final Grade\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b28d9",
   "metadata": {},
   "source": [
    "Students with no support are performing slightly higher (likely because students with support are already at risk)\n",
    "So when thinking about it , students with suport are performing slighly better but still it is not that significant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e84aa",
   "metadata": {},
   "source": [
    "Failures vs Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x='failures',y='G3',data=df_corr)\n",
    "plt.title(\"Final Grade vs Past Failures\")\n",
    "plt.xlabel(\"Past Failures\")\n",
    "plt.ylabel(\"Final Grade\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ffc78",
   "metadata": {},
   "source": [
    "Here there is a clear negative correlation as faiulres increase , the final grade decreases "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b50289",
   "metadata": {},
   "source": [
    "Sex vs Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6204da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x='sex',y='G3',data=df_corr)\n",
    "plt.title(\"Final Grade vs Sex\",fontsize=14)\n",
    "plt.xlabel(\"Sex (F=Female, M=male)\")\n",
    "plt.ylabel(\"Final Grade\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.groupby('sex')['G3'].mean().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7b6c0",
   "metadata": {},
   "source": [
    "Looks like female are slightly overperforming but it is still not that significant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a959d0",
   "metadata": {},
   "source": [
    "Lifestyle score vs outcomes\n",
    "(Lifestyle score will combine the scores of goout, health and freetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01936f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr['Lifestyle_score']=(df_corr['goout'] + df_corr['freetime'] + df_corr['health'])/3\n",
    "def categorize_lifestyle(score):\n",
    "    if score <2.5: \n",
    "        return 'Low'\n",
    "    elif score <3.5:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "df_corr['lifestyle_group']=df_corr['Lifestyle_score'].apply(categorize_lifestyle)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x='lifestyle_group',y='G3',data=df_corr)\n",
    "plt.title(\"Final Grade vs Lifestyle group\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4fe925",
   "metadata": {},
   "source": [
    "Let's calculate the mean to get a good feel about the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f15982",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.groupby('lifestyle_group')['G3'].mean().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5addf7bb",
   "metadata": {},
   "source": [
    "so there is not much of a difference between them and looks like they are not strong indicators of the final grade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19da3e5",
   "metadata": {},
   "source": [
    "Hypotheses \n",
    "1. Absences are negatively correlated with final grades \n",
    "   Evidence: The heatmap shows -0.09\n",
    "   The scatter plot shows no direct relation \n",
    "   Conclusion: Hypothesis Supported but weak effect\n",
    "2. Students with more past failures have lower final grades\n",
    "   Evidence: the heatmap shows a correlation of -0.39 \n",
    "   Students with more than 2 failures have lower grades from the box plot \n",
    "   Conclusion: Hypothesis Supported with failures being a meaningful predictor\n",
    "3. Sex isn't correlated to final grade\n",
    "   Evidence:The boxplot is not that different \n",
    "   Means are not that different \n",
    "   Conclusion: Hypothesis Supported\n",
    "4. Study time is strongly correlated to final grade\n",
    "    Evidence: After 2 hours , there is not that much of a difference. \n",
    "    Conclusion: Hypothesis Rejected (Weak effect)\n",
    "5. Students with  no school support  outperform students with school support\n",
    "    Evidence: Although the boxplot of students with no school support has better boxplot but when thinking about it school support is for students who actually need it. \n",
    "    Conclusion: Hypothesis Rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120bc0a7",
   "metadata": {},
   "source": [
    "Before going on to the next module , for a one line summary \n",
    "acadmic history is far more infleunetial on final outcomes than behavioral , demographic , or support-related factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8f8ce",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea5584",
   "metadata": {},
   "source": [
    "Above in the EDA , I already did the correlation heatmap,scatter plot of absences with interpertation, boxplot of studytime and boxplot of schoolsup ,  but I will put it here for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9486b8",
   "metadata": {},
   "source": [
    "Final Grade vs Absencess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa80ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x='absences',y='G3',data=df_corr,alpha=0.6)\n",
    "plt.title(\"Final Grade vs Absences\",fontsize=14)\n",
    "plt.ylabel(\"Absences\")\n",
    "plt.xlabel(\"Final Grade\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb2679",
   "metadata": {},
   "source": [
    "Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21211be",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=df_corr[num_features].corr()\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr,annot=True,fmt=\".2f\",cmap=\"coolwarm\",cbar=True,square=True)\n",
    "plt.title(\"Correlation Heatmap of Numeric Features\",fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8fd0e3",
   "metadata": {},
   "source": [
    "Boxplot of school support vs final grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf78e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x='schoolsup',y='G3',data=df_corr)\n",
    "plt.title(\"Final Grade vs School Support\",fontsize=14)\n",
    "plt.xlabel(\"School Support (Yes/No)\")\n",
    "plt.ylabel(\"Final Grade\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b14979",
   "metadata": {},
   "source": [
    "Histogram of 3+ numeric variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b84abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_corr['G3'],bins=15,kde=True)\n",
    "plt.title(f\"Distribution of G3\",fontsize=14)\n",
    "plt.xlabel('G3')\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540e1ba",
   "metadata": {},
   "source": [
    "So Final grade distribution looks rightly skewed and most grades range from 10 to 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_corr['absences'],bins=15,kde=True)\n",
    "plt.title(f\"Distribution of absences\",fontsize=14)\n",
    "plt.xlabel('absences')\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78916cc0",
   "metadata": {},
   "source": [
    "Looks like most students don't miss classes as 0 is the most frequent value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_corr['studytime'],bins=15)\n",
    "plt.title(\"Distribution of Study time\",fontsize=14)\n",
    "plt.xlabel(\"Study Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba3442c",
   "metadata": {},
   "source": [
    "So looks like most students study between 2 hours to 2.25 hours "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c5e502",
   "metadata": {},
   "source": [
    "### Unsupervised Learning (K-Means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f003be09",
   "metadata": {},
   "source": [
    "As K-means is an unsupervised clustering algorithm, we will  create a feature set for behavior segmentation (which means academics don't matter here , so we will use the df_without and y_without )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090555af",
   "metadata": {},
   "source": [
    "Feature set for behavior segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9527faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "behavioral_features=['studytime','absences','goout','freetime','famsup','schoolsup']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73903bf5",
   "metadata": {},
   "source": [
    "Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac650b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "X=df_without[behavioral_features]\n",
    "inertia=[]\n",
    "K=range(2,11)\n",
    "for k in K:\n",
    "    kmeans=KMeans(n_clusters=k,random_state=43,n_init=10)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "plt.plot(K,inertia,'bo-')\n",
    "plt.xlabel('k (Number of clusters)')\n",
    "plt.ylabel('Inertia (SSE)')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc8c77",
   "metadata": {},
   "source": [
    "According to the elbow method, a good number of clusters would be 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette=[]\n",
    "for k in K:\n",
    "    kmeans=KMeans(n_clusters=k,random_state=42,n_init=10)\n",
    "    labels=kmeans.fit_predict(X)\n",
    "    silhouette.append(silhouette_score(X,labels))\n",
    "plt.plot(K,silhouette,'-bo')\n",
    "plt.xlabel('K (number of clusters)')\n",
    "plt.ylabel('Average Silhouette Score')\n",
    "plt.title('Silhouette Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e15022a",
   "metadata": {},
   "source": [
    "For silhouette method , the best K is 4 \n",
    "The difference between elbow method and silhouette score is that elbow method favors more clusters since it measures how well points fit within clusters , yet the silhouette score balances cohesion vs sepration which means clusters are best seperated and consistent at the k with the highest silhouette score. We will cluster at k=4 and k=6 and look at the difference between clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f34385",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for k in [4, 6]:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X)   \n",
    "    \n",
    "    df_temp = df_without.copy()\n",
    "    df_temp[f'cluster_{k}'] = labels\n",
    "    \n",
    "    cluster_profile = df_temp.groupby(f'cluster_{k}').agg(\n",
    "        size=(f'cluster_{k}', 'size'),\n",
    "        avg_studytime=('studytime', 'mean'),\n",
    "        avg_absences=('absences', 'mean'),\n",
    "        avg_goout=('goout', 'mean'),\n",
    "        avg_freetime=('freetime', 'mean'),\n",
    "        avg_famsup=('famsup', 'mean'),\n",
    "        avg_schoolsup=('schoolsup', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    results[k] = cluster_profile\n",
    "\n",
    "print(\"K=4 Clusters:\\n\")\n",
    "results[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5759b013",
   "metadata": {},
   "source": [
    "let's analyze clusters \n",
    "- Cluster 0:low studytime , low absences and low solializing -> Quiet/Average students\n",
    "- Cluster 1:high absences, moderate goout and low studytime -> The skippers\n",
    "- Cluster 2:very high study time , low absences, low social-> Diligent Students\n",
    "- Cluster 3->low studytime , low abences , but very high goout and freetime -> Social/Party Group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651106ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nK=6 Clusters:\\n\")\n",
    "results[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d3d0f",
   "metadata": {},
   "source": [
    "- Cluster 0: very low studytime , very low absences , very low freetime->Quiet students\n",
    "- Cluster 1:low studytime,low absences, low goout , moderate freetime-> Quiet Students 2\n",
    "- Cluster 2: low studytime, low absences, very high goout/freetime -> Social party group \n",
    "- Cluster 3:very high studytime , low absences, low goout-> Diligent Students\n",
    "- Cluster 4 :low studytime , very high absences ,High goout/freetime-> The skippers\n",
    "- Cluster 5: low study time, very high absences , low goout/freetime-> The skippers 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8cbfe6",
   "metadata": {},
   "source": [
    "Notice how the clusters are repeated when k was 6. ( 2 Groups for skippers and 2 groups for quiet)\n",
    "K=4 is the better option since each group represents a clear behavioral archetype and are better seperated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X)   \n",
    "    \n",
    "df_temp = df_without.copy()\n",
    "df_temp[f'cluster_{4}'] = labels\n",
    "df_temp['G3']=y_with_rg\n",
    "    \n",
    "cluster_profile = df_temp.groupby(f'cluster_4').agg(\n",
    "        size=(f'cluster_4', 'size'),\n",
    "        avg_studytime=('studytime', 'mean'),\n",
    "        avg_absences=('absences', 'mean'),\n",
    "        avg_goout=('goout', 'mean'),\n",
    "        avg_freetime=('freetime', 'mean'),\n",
    "        avg_famsup=('famsup', 'mean'),\n",
    "        avg_schoolsup=('schoolsup', 'mean'),\n",
    "        avg_G3=('G3','mean')\n",
    "    ).reset_index()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "kmeans=KMeans(n_clusters=4,random_state=42,n_init=10)\n",
    "labels=kmeans.fit_predict(X)\n",
    "pca=PCA(n_components=2,random_state=42)\n",
    "x_pca=pca.fit_transform(X)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x_pca[:,0],x_pca[:,1],c=labels,cmap='tab10',s=40,alpha=0.7)\n",
    "centers_pca=pca.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centers_pca[:,0],centers_pca[:,1],c='red',marker='X',s=200,label='Centroids')\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.title(\"K-means Clusters (K=4)- PCA reduced\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813fc6b",
   "metadata": {},
   "source": [
    "Here this is a visualization for cluters. Notice that they do overlap as PCA reduces multi-dimensional behavioral features (6D) into two dimesnions (2D), yet each cluster is meaningfully distinct in terms of their centroids and average behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26e3ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "g3_comparison = df_temp.groupby('cluster_4')['G3'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca872b",
   "metadata": {},
   "source": [
    "Compare  average G3 across clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915b6a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "g3_comparison.plot(kind='bar',color='skyblue',edgecolor='black')\n",
    "plt.title(\"Average G3 per Cluster (K=4)\",fontsize='14')\n",
    "plt.xlabel(\"Cluster\",fontsize=12)\n",
    "plt.ylabel(\"Average G3\",fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylim(0,20)\n",
    "for i, val in enumerate(g3_comparison):\n",
    "    plt.text(i,val+0.2,f\"{val:.2f}\",ha='center',fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ccd06",
   "metadata": {},
   "source": [
    "So looks like Cluster 2 (The diligent students) has the highes average G3 grade and they clearly perform best. \n",
    "Morever Cluster 1 (The skippers) have the least performance which links skipping class to poor outcomes.\n",
    "Finally Clusters 0 and 3 are middle-of-the-road students with cluster 3 being more socially active."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7a267",
   "metadata": {},
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342068b3",
   "metadata": {},
   "source": [
    "Define Target: it will be a 3-class risk that was already created using the function risk_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b629799",
   "metadata": {},
   "source": [
    "Train at least 3 algorithms \n",
    "I will use \n",
    "- Random Forest\n",
    "- Logistic Regressoin \n",
    "- Decision Tree\n",
    "\n",
    "\n",
    "but first we split the data \n",
    "for each algorithm there is a pipeline that will be followed \n",
    "1. Train the model \n",
    "2. Use cross validation and perform basic hyperparameter tuning \n",
    "3. Report full metrics\n",
    "4. Interpret Model \n",
    "\n",
    "Finally a table will be created at the end to be able to see the differences.\n",
    "We also need to make sure that this is done for the two data-leakage variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323c89a",
   "metadata": {},
   "source": [
    "Dataset with G1 and G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2939fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#stratify here keeps class balance\n",
    "X_train,X_test,y_train,y_test=train_test_split(df_with,y_with_cls,test_size=0.2,random_state=42,stratify=y_with_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0fbe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ac982",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7e01ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c54f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6313a69f",
   "metadata": {},
   "source": [
    "Now that we have 3 models and 2 datasets to train we are going to build a function called evaluate_model which will train the model , evaluate it on test data and returns a dictionary of metrics. \n",
    "We will also run GridSearch for all the models to get the best values for the hyper-parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6a6174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def evaluate_model(model,X_train,X_test,y_train,y_test,cv=5):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred=model.predict(X_test)\n",
    "    metrics={\n",
    "        \"accuracy\":accuracy_score(y_test,y_pred),\n",
    "        \"precision_score_macro\":precision_score(y_test,y_pred,average=\"macro\"),\n",
    "        \"recall_score_macro\":recall_score(y_test,y_pred,average=\"macro\"),\n",
    "        \"f1_score_macro\":f1_score(y_test,y_pred,average=\"macro\"),\n",
    "        \"cv_score_macro\":np.mean(cross_val_score(model,X_train,y_train,cv=cv,scoring=\"f1_macro\"))\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc39c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "tuning_results = {\"df_with\": {}, \"df_without\": {}}\n",
    "results = {\"df_with\": {}, \"df_without\": {}}\n",
    "\n",
    "datasets = {\n",
    "    \"df_with\": (df_with, y_with_cls),\n",
    "    \"df_without\": (df_without, y_without_cls)\n",
    "}\n",
    "\n",
    "param_grids = { \n",
    "    \"LogisticRegression\": {\"C\": [0.01, 0.1, 1, 10, 100]}, \n",
    "    \"RandomForest\": { \n",
    "        \"n_estimators\": [100, 200, 500], \n",
    "        \"max_depth\": [None, 10, 20], \n",
    "        \"min_samples_split\": [2, 5, 10] \n",
    "    }, \n",
    "   \"DecisionTree\": {\n",
    "        \"max_depth\": [None, 5, 10, 20],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"criterion\": [\"gini\", \"entropy\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {  \n",
    "    \"LogisticRegression\": LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=1000, random_state=42),  \n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42),  \n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42)  \n",
    "}\n",
    "\n",
    "for name, (X, y) in datasets.items():  \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    for model_name, model in models.items(): \n",
    "        print(f\"Running {model_name} on {name} ...\") \n",
    "        \n",
    "        grid = GridSearchCV(\n",
    "            model, param_grids[model_name], \n",
    "            cv=5, scoring=\"f1_macro\", n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Train GridSearch\n",
    "        grid.fit(X_train, y_train)\n",
    "        \n",
    "        # Store tuning results\n",
    "        tuning_results[name][model_name] = {\n",
    "            \"best_params\": grid.best_params_,\n",
    "            \"best_cv_f1_macro\": grid.best_score_\n",
    "        }\n",
    "        \n",
    "        \n",
    "        best_model = grid.best_estimator_\n",
    "        metrics = evaluate_model(best_model, X_train, X_test, y_train, y_test)\n",
    "        results[name][model_name] = metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c1146",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95350763",
   "metadata": {},
   "source": [
    "## Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_rows = []\n",
    "for dataset, models in tuning_results.items():\n",
    "    for model_name, vals in models.items():\n",
    "        tuning_rows.append({\n",
    "            \"Dataset\": dataset,\n",
    "            \"Model\": model_name,\n",
    "            \"Best Params\": vals[\"best_params\"],\n",
    "            \"Best CV F1\": vals[\"best_cv_f1_macro\"]\n",
    "        })\n",
    "tuning_df = pd.DataFrame(tuning_rows)\n",
    "\n",
    "result_rows = []\n",
    "for dataset, models in results.items():\n",
    "    for model_name, vals in models.items():\n",
    "        row = {\"Dataset\": dataset, \"Model\": model_name}\n",
    "        row.update(vals)  \n",
    "        result_rows.append(row)\n",
    "results_df = pd.DataFrame(result_rows)\n",
    "\n",
    "final_df = pd.merge(tuning_df, results_df, on=[\"Dataset\", \"Model\"])\n",
    "\n",
    "\n",
    "final_df = final_df[\n",
    "    [\"Dataset\", \"Model\", \"Best Params\", \"Best CV F1\",\n",
    "     \"accuracy\", \"precision_score_macro\", \"recall_score_macro\",\n",
    "     \"f1_score_macro\", \"cv_score_macro\"]\n",
    "]\n",
    "\n",
    "\n",
    "for col in [\"Best CV F1\", \"accuracy\", \"precision_score_macro\", \n",
    "            \"recall_score_macro\", \"f1_score_macro\", \"cv_score_macro\"]:\n",
    "    final_df[col] = final_df[col].round(3)\n",
    "\n",
    "final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02df8b98",
   "metadata": {},
   "source": [
    "With leakage all models perform well with logistic regression achieving the highest precision. Random Forest also outperformes others in CV F1 while DecisionTree is more interpretable.\n",
    "Yet without leakage the performance drops significantly. Logisitic regression has the better F1 score compared to others , while RandomForest shows high precision but low recall which makes it clear that it predictes fewer positives correctly \n",
    "So final observation is that models overfit when leakage exist and removal of leakage allows more generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0538585d",
   "metadata": {},
   "source": [
    "Now we can look for interpretiability \n",
    "for logistic regression it would be the coefficients where large  positive coefficents show features strong associated with a specific risk class and large negative coeeficeints reduces the likelihood of that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4958aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_coeffs(X, y, best_params, dataset_name, top_n=10):\n",
    "    logreg = LogisticRegression(\n",
    "        **best_params,\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    logreg.fit(X, y)\n",
    "\n",
    "    coeffs = pd.DataFrame(\n",
    "        logreg.coef_.T,\n",
    "        index=X.columns,\n",
    "        columns=[f\"class_{cls}\" for cls in logreg.classes_]\n",
    "    )\n",
    "\n",
    "    coeffs_sorted = coeffs.sort_values(by=coeffs.columns[0], ascending=False).head(top_n)\n",
    "    coeffs_sorted[\"dataset\"] = dataset_name\n",
    "    return coeffs_sorted\n",
    "\n",
    "top_with = get_top_coeffs(df_with, y_with_cls, tuning_results[\"df_with\"][\"LogisticRegression\"][\"best_params\"], \"df_with\")\n",
    "top_without = get_top_coeffs(df_without, y_without_cls, tuning_results[\"df_without\"][\"LogisticRegression\"][\"best_params\"], \"df_without\")\n",
    "\n",
    "comparison = pd.concat([top_with, top_without])\n",
    "\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abefc205",
   "metadata": {},
   "source": [
    "In df_with, G1 and G2 strongly and positively influence the probability of being in the High-risk class. The coefficients for these features are much larger than others, suggesting data leakage, as these features dominate the model.\n",
    "In contrast, for df_without, the coefficients are smaller and more balanced, reflecting more realistic predictors. Here, features like school_MS and schoolsup positively influence High risk, indicating that non-grade-related factors are driving the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8995e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_reset = comparison.reset_index().rename(columns={\"index\": \"Feature\"})\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20,6), sharey=True)\n",
    "\n",
    "for i, cls in enumerate([\"class_High\", \"class_Low\", \"class_Medium\"]):\n",
    "    plot_data = comparison_reset[[\"Feature\", cls, \"dataset\"]].rename(columns={cls: \"Coefficient\"})\n",
    "    \n",
    "    sns.barplot(\n",
    "        data=plot_data,\n",
    "        x=\"Feature\",\n",
    "        y=\"Coefficient\",\n",
    "        hue=\"dataset\",\n",
    "        palette=\"Set2\",\n",
    "        ax=axes[i]\n",
    "    )\n",
    "    \n",
    "    axes[i].set_title(f\"LogReg Coefficients for {cls}\", fontsize=14)\n",
    "    axes[i].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.suptitle(\"Logistic Regression Coefficients Comparison (df_with vs df_without)\", fontsize=16, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c92e3c8",
   "metadata": {},
   "source": [
    "Random Forest would have feature importance where features with highest importance are the main drivers of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_with=RandomForestClassifier(**tuning_results[\"df_with\"][\"RandomForest\"][\"best_params\"],random_state=42)\n",
    "rf_with.fit(df_with,y_with_cls)\n",
    "importances_with=pd.DataFrame({\n",
    "    \"feature\":df_with.columns,\n",
    "    \"importance\":rf_with.feature_importances_\n",
    "}).sort_values(\"importance\",ascending=False)\n",
    "rf_without=RandomForestClassifier(**tuning_results[\"df_without\"][\"RandomForest\"][\"best_params\"],random_state=42)\n",
    "rf_without.fit(df_without,y_without_cls)\n",
    "importances_without=pd.DataFrame({\n",
    "    \"feature\":df_without.columns,\n",
    "    \"importance\":rf_without.feature_importances_\n",
    "}).sort_values(\"importance\",ascending=False)\n",
    "importances_with[\"dataset\"] = \"df_with\"\n",
    "importances_without[\"dataset\"] = \"df_without\"\n",
    "\n",
    "\n",
    "rf_importances = pd.concat([importances_with, importances_without])\n",
    "\n",
    "\n",
    "top_n = 10\n",
    "top_features = (\n",
    "    rf_importances.groupby(\"feature\")[\"importance\"]\n",
    "    .max()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(top_n)\n",
    "    .index\n",
    ")\n",
    "plot_data = rf_importances[rf_importances[\"feature\"].isin(top_features)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(\n",
    "    data=plot_data,\n",
    "    x=\"feature\",\n",
    "    y=\"importance\",\n",
    "    hue=\"dataset\",\n",
    "    palette=\"Set2\"\n",
    ")\n",
    "\n",
    "plt.title(\"Top Random Forest Feature Importances (df_with vs df_without)\", fontsize=14)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.legend(title=\"Dataset\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b0d9e7",
   "metadata": {},
   "source": [
    "The Random Forest feature importance plot shows that in df_with, features like avg_grade and G2 dominate the model, confirming the presence of data leakage, as these grade-related features almost fully determine the prediction.\n",
    "In contrast, for df_without, the importance is more evenly distributed among features like failures, age, health, and famrel, indicating that the model is now relying on more realistic and meaningful predictors. This demonstrates how removing leakage changes the modelâ€™s behavior and improves interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb99a4",
   "metadata": {},
   "source": [
    "Now for Decision Tree we can try to visualize the best decision tree created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da87c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "best_dt_with = DecisionTreeClassifier(random_state=42, **tuning_results[\"df_with\"][\"DecisionTree\"][\"best_params\"])\n",
    "best_dt_with.fit(df_with, y_with_cls)\n",
    "\n",
    "best_dt_without = DecisionTreeClassifier(random_state=42, **tuning_results[\"df_without\"][\"DecisionTree\"][\"best_params\"])\n",
    "best_dt_without.fit(df_without, y_without_cls)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(24, 8)) \n",
    "\n",
    "\n",
    "tree.plot_tree(\n",
    "    best_dt_with,\n",
    "    feature_names=df_with.columns,\n",
    "    class_names=[str(cls) for cls in best_dt_with.classes_],\n",
    "    filled=True,\n",
    "    max_depth=3,          \n",
    "    fontsize=10,          \n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Decision Tree (df_with)\")\n",
    "\n",
    "\n",
    "tree.plot_tree(\n",
    "    best_dt_without,\n",
    "    feature_names=df_without.columns,\n",
    "    class_names=[str(cls) for cls in best_dt_without.classes_],\n",
    "    filled=True,\n",
    "    max_depth=3,\n",
    "    fontsize=10,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Decision Tree (df_without)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e6f0a",
   "metadata": {},
   "source": [
    "For the left decision tree , the average grade dominates the top node which  suggest that G1 and G2 are strong predictors, yet on the right the top node is failures and relies more on realistic student information. that concluds that df_with suffers from data leakahge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd30202",
   "metadata": {},
   "source": [
    "### Storytelling and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba0256",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aijack_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
